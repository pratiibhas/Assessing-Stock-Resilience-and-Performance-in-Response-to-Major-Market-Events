{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae439acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# List of (date, article_url) tuples for the example articles\n",
    "articles = [\n",
    "    (\"2018-12-14\", \"https://www.moneycontrol.com/news/business/markets/year-in-review-10-key-events-that-charted-market-direction-in-2018-3274721.html\"),\n",
    "    (\"2018-12-23\", \"https://www.businesstoday.in/markets/stocks/story/sensex-nifty-stock-market-nifty-50-rbi-yearend-2018-126099-2018-12-24\")\n",
    "]\n",
    "\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract main article text - this depends on site's HTML structure\n",
    "        # Common tags: article, div with class \"article\", \"content\", \"post-content\", etc.\n",
    "        \n",
    "        # Try multiple common patterns\n",
    "        article_text = \"\"\n",
    "        if soup.find(\"article\"):\n",
    "            article_text = ' '.join(p.get_text() for p in soup.find(\"article\").find_all(\"p\"))\n",
    "        elif soup.find(\"div\", class_=\"content\"):\n",
    "            article_text = ' '.join(p.get_text() for p in soup.find(\"div\", class_=\"content\").find_all(\"p\"))\n",
    "        elif soup.find(\"div\", class_=\"article\"):\n",
    "            article_text = ' '.join(p.get_text() for p in soup.find(\"div\", class_=\"article\").find_all(\"p\"))\n",
    "        else:\n",
    "            # fallback: get all paragraphs\n",
    "            article_text = ' '.join(p.get_text() for p in soup.find_all(\"p\"))\n",
    "\n",
    "        return article_text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Collect data\n",
    "data = []\n",
    "for date, url in articles:\n",
    "    print(f\"Scraping: {url}\")\n",
    "    content = get_article_text(url)\n",
    "    data.append({\"date\": date, \"article_link\": url, \"article_content\": content})\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n",
    "\n",
    "# Optionally save to CSV\n",
    "df.to_csv(\"articles_2018.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
